{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bibekali/anaconda3/envs/hansol-deco-env-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "# from llama_index.core.indices.base import BaseIndex\n",
    "\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "class Retriever():\n",
    "    \n",
    "    def __init__(self, index: VectorStoreIndex, embed_model: BaseEmbedding, reranker: FlagEmbeddingReranker=None, query_engine: RetrieverQueryEngine=None):\n",
    "        self.index = index\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = reranker\n",
    "        self.query_engine = query_engine\n",
    "\n",
    "    @classmethod\n",
    "    def load_index_from_disk(cls, PERSIST_DIR, embed_model_name=\"BAAI/bge-m3\", pooling=\"mean\"):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "        embed_model = HuggingFaceEmbedding(\n",
    "                        model_name=embed_model_name,\n",
    "                        pooling=pooling # \"cls\" is default\n",
    "                    )\n",
    "\n",
    "        index = load_index_from_storage(storage_context, embed_model=embed_model, llm=None)\n",
    "        return cls(index=index, embed_model=embed_model)\n",
    "\n",
    "    def load_reranker(self, top_n=3, reranker_id=\"Dongjin-kr/ko-reranker\"):\n",
    "        self.reranker = FlagEmbeddingReranker(\n",
    "                    top_n=top_n,\n",
    "                    model=reranker_id,\n",
    "                )\n",
    "\n",
    "    def load_retriever_query_engine(self, top_k):\n",
    "        Settings.llm = None\n",
    "        retriever_query_engine = self.index.as_query_engine(similarity_top_k=top_k, node_postprocessors=[self.reranker], verbose=True)\n",
    "        self.query_engine = retriever_query_engine\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_k_relevant_documents_with_reranker(self, query):\n",
    "        relevent_docs = self.query_engine.query(query)\n",
    "        return relevent_docs        \n",
    "\n",
    "        \n",
    "    def get_context_from_relevant_documents(self, relevent_docs):\n",
    "        context_list = []\n",
    "        for r in relevent_docs.source_nodes:\n",
    "            if r.score > 0:\n",
    "                context_list.append(r.text)\n",
    "        return context_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = Retriever.load_index_from_disk(PERSIST_DIR=\"../index/train-vector-index-storage-chunk-size-1295\", embed_model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Retriever at 0x73fa8c64f640>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.load_reranker(top_n=3, reranker_id=\"Dongjin-kr/ko-reranker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "retriever.load_retriever_query_engine(top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x73fa8c67f490>,\n",
       " 'embed_model': HuggingFaceEmbedding(model_name='BAAI/bge-m3', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x73fa89a8ba90>, tokenizer_name='BAAI/bge-m3', max_length=8194, pooling=<Pooling.MEAN: 'mean'>, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None),\n",
       " 'reranker': FlagEmbeddingReranker(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x73fa89a8ba90>, model='Dongjin-kr/ko-reranker', top_n=3, use_fp16=False),\n",
       " 'query_engine': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x73fb4032c130>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_000, 면진장치가 뭐야?, 면진장치에 사용되는 주요 기술은 무엇인가요?, 건축구조, 면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다., 면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다., 면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다., 면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다., 면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = retriever.get_k_relevant_documents_with_reranker(query = \"면진장치가 뭐야?\")\n",
    "context_list = retriever.get_context_from_relevant_documents(relevant_docs)\n",
    "for c in context_list:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Augment(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompt(cls, mode, retriever, question, prev_q=\"\"):\n",
    "        # prev_q is a must needed for some questions like this: What is the biggest cause of plaster revision? And please tell me how to solve this.”\n",
    "        if mode==\"gpu-solar\":\n",
    "            INSTRUCTION_PROMPT_TEMPLATE = \"\"\"\\\n",
    "            ### System:\n",
    "            벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. <질문>의 의도를 파악하여 정확하게 <보고서>만을 기반으로 답변하세요.\n",
    "\n",
    "            ### User:\n",
    "            <보고서>\n",
    "            {CONTEXT}\n",
    "            </보고서>\n",
    "            지침사항을 반드시 지키고, <보고서>를 기반으로 <질문>에 답변하세요.\n",
    "            <질문>\n",
    "            {QUESTION}\n",
    "            </질문>\n",
    "\n",
    "            ### Assistant:\n",
    "            \"\"\"\n",
    "        else:\n",
    "            INSTRUCTION_PROMPT_TEMPLATE = \"\"\"\\\n",
    "            <start_of_turn>user\n",
    "            벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. <질문>의 의도를 파악하여 정확하게 <보고서>만을 기반으로 답변하세요.\n",
    "            보고서: {CONTEXT}\n",
    "            질문: {QUESTION}\n",
    "            <start_of_turn>model\n",
    "            \"\"\"\n",
    "\n",
    "        response_1 = retriever.query_engine.query(question)\n",
    "\n",
    "        context_list = []\n",
    "        for r in response_1.source_nodes:\n",
    "            # print(r.score)\n",
    "            if r.score > 0:\n",
    "                if r.score <= 4 and len(context_list) >= 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    context_list.append(r.text)\n",
    "\n",
    "        # Special case when the follow up question is junk\n",
    "        if len(context_list) == 0:\n",
    "            response_2 = retriever.query_engine.query(prev_q + \" \" + question)\n",
    "            for r in response_2.source_nodes:\n",
    "                if r.score > 0:\n",
    "                    context_list.append(r.text)\n",
    "\n",
    "        context = prev_q + \"\\n\\n\".join(context_list)\n",
    "        # context = prev_q + \"\\n\\n\".join(context_list + [question])\n",
    "\n",
    "        prompt = INSTRUCTION_PROMPT_TEMPLATE.format(CONTEXT=context, QUESTION=question)   \n",
    "\n",
    "        return cls(prompt=prompt)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            <start_of_turn>user\n",
      "            벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. <질문>의 의도를 파악하여 정확하게 <보고서>만을 기반으로 답변하세요.\n",
      "            보고서: TRAIN_000, 면진장치가 뭐야?, 면진장치에 사용되는 주요 기술은 무엇인가요?, 건축구조, 면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다., 면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다., 면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다., 면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다., 면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.\n",
      "            질문: 면진장치가 뭐야?\n",
      "            <start_of_turn>model\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "question = \"면진장치가 뭐야?\"\n",
    "augment = Augment.get_prompt(\"cpu\", retriever, question)\n",
    "print(augment.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "#   bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "#   pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "# text = '근육이 커지기 위해서는'\n",
    "# text = \"\"\"\n",
    "# 벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. '질문'의 의도를 파악하여 정확하게 '보고서'만을 기반으로 답변하세요.\n",
    "# 보고서: TRAIN_000, 면진장치가 뭐야?, 면진장치에 사용되는 주요 기술은 무엇인가요?, 건축구조, 면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다., 면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다., 면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다., 면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다., 면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.\n",
    "# 지침사항을 반드시 지키고, '보고서'를 기반으로 '질문'에 답변하세요.\n",
    "# 질문: 면진장치가 뭐야?\n",
    "# \"\"\"\n",
    "# input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "# gen_ids = model.generate(input_ids,\n",
    "#                          max_length=500,\n",
    "#                          repetition_penalty=2.0,\n",
    "#                          pad_token_id=tokenizer.pad_token_id,\n",
    "#                          eos_token_id=tokenizer.eos_token_id,\n",
    "#                          bos_token_id=tokenizer.bos_token_id,\n",
    "#                          use_cache=True)\n",
    "# generated = tokenizer.decode(gen_ids[0])\n",
    "# print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/bibekali/dacon-hansol-deco-challenge/hansolrag/model_checkpoints/gemma/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =   500.25 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 617\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"/home/bibekali/dacon-hansol-deco-challenge/hansolrag/model_checkpoints/gemma/gemma-2b-it.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_cpp.llama.Llama"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   51384.35 ms\n",
      "llama_print_timings:      sample time =      79.03 ms /    42 runs   (    1.88 ms per token,   531.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51382.42 ms /   466 tokens (  110.26 ms per token,     9.07 tokens per second)\n",
      "llama_print_timings:        eval time =   18838.05 ms /    41 runs   (  459.46 ms per token,     2.18 tokens per second)\n",
      "llama_print_timings:       total time =   71302.94 ms /   507 tokens\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\\\n",
    "<start_of_turn>user\n",
    "벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. <질문>의 의도를 파악하여 정확하게 <보고서>만을 기반으로 답변하세요.\n",
    "보고서: TRAIN_000, 면진장치가 뭐야?, 면진장치에 사용되는 주요 기술은 무엇인가요?, 건축구조, 면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다., 면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다., 면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다., 면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다., 면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.\n",
    "질문: 면진장치가 뭐야?<end_of_turn>\n",
    "<start_of_turn>model\\\n",
    "\"\"\"\n",
    "\n",
    "output= llm(text, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-f829e1f2-747c-4b4d-b736-5be878a93826',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711894400,\n",
       " 'model': '/home/bibekali/dacon-hansol-deco-challenge/hansolrag/model_checkpoints/gemma/gemma-2b-it.gguf',\n",
       " 'choices': [{'text': '**면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다.**',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 466, 'completion_tokens': 41, 'total_tokens': 507}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '**면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다.**'\n",
    "a.lstrip('*').rstrip('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, pipeline, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "class Generator(BaseModel):\n",
    "    mode: str\n",
    "    model: Any\n",
    "    tokenizer: Any\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config_path: str):\n",
    "        import yaml\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        options = config.get('options', [])\n",
    "        if not options:\n",
    "            raise ValueError(\"No options found in the config\")\n",
    "        \n",
    "        chosen_option = options[0]\n",
    "        mode = chosen_option['name']\n",
    "\n",
    "        if chosen_option is None:\n",
    "            raise ValueError(\"Chosen mode not found in the config\")\n",
    "\n",
    "        model = cls.get_generative_model(cls, mode, chosen_option['model_id'], chosen_option.get('quantized', False))\n",
    "        tokenizer = cls.get_tokenizer(cls, mode, chosen_option['model_id'], chosen_option.get('tokenizer_config', {}))\n",
    "\n",
    "        return cls(\n",
    "            mode=mode,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_generative_model(self, mode: str, model_id: str, quantized: str):\n",
    "        if mode == \"gpu-solar\":\n",
    "            quantized = quantized.lower() == 'true'\n",
    "            if quantized:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.half\n",
    "                )\n",
    "            else:\n",
    "                bnb_config = None\n",
    "\n",
    "            model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                quantization_config = bnb_config,\n",
    "                device_map=\"auto\" #\"cuda\" if low_cpu_mem_usage=False\n",
    "            )\n",
    "\n",
    "            merged_model = model.merge_and_unload()\n",
    "            return merged_model\n",
    "        \n",
    "        elif mode == \"cpu-gpt2\":\n",
    "            return GPT2LMHeadModel.from_pretrained(model_id)\n",
    "    \n",
    "        elif mode == \"cpu-gemini\":\n",
    "            return Llama(model_path=model_id)\n",
    "\n",
    "\n",
    "    def get_tokenizer(self, mode, model_id, tokenizer_config):\n",
    "        if mode==\"gpu-solar\":\n",
    "            if tokenizer_config.get(\"eos_token\", None) is not None:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, eos_token=tokenizer_config.get(\"eos_token\"))\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "                \n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"right\"\n",
    "            return tokenizer\n",
    "        \n",
    "        elif mode==\"cpu-gpt2\":\n",
    "            tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id, **tokenizer_config)\n",
    "            return tokenizer\n",
    "        \n",
    "        elif mode==\"cpu-gemini\":\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def generate_text(self, text):\n",
    "        if self.mode==\"gpu-solar\":\n",
    "            pipeline_ = pipeline(\"text-generation\", self.model, tokenizer=self.tokenizer)\n",
    "            output = pipeline_(text,\n",
    "                    min_new_tokens=20,\n",
    "                    max_new_tokens=256,\n",
    "                    top_p=0.98,\n",
    "                    top_k=50,\n",
    "                    temperature=0.9,\n",
    "                    return_full_text=False,\n",
    "                    eos_token_id = self.tokenizer.eos_token_id\n",
    "                    )\n",
    "            return output[0].get('generated_text').lstrip().rstrip()\n",
    "        \n",
    "        elif self.mode==\"cpu-gpt2\":\n",
    "            pipeline_ = pipeline(\"text-generation\", self.model, tokenizer=self.tokenizer)\n",
    "            output = pipeline_(text,\n",
    "                    min_new_tokens=20,\n",
    "                    max_new_tokens=256,\n",
    "                    top_p=0.98,\n",
    "                    top_k=50,\n",
    "                    temperature=0.9,\n",
    "                    return_full_text=False,\n",
    "                    eos_token_id = self.tokenizer.eos_token_id\n",
    "                    )\n",
    "            return output[0].get('generated_text').lstrip().rstrip()\n",
    "        \n",
    "        elif self.mode==\"cpu-gemini\":\n",
    "            output = self.model(text, max_tokens=256)\n",
    "            return output.get('choices')[0].get('text').lstrip('*').rstrip('*')\n",
    "        \n",
    "\n",
    "    def get_output(self, retriever, query_list):\n",
    "        responses = []\n",
    "\n",
    "        for idx, question in tqdm(enumerate(query_list)):\n",
    "            questions = re.split('[?!.]', question)\n",
    "            seperate_output = []\n",
    "            prev_q = \"\"\n",
    "            for q in questions:\n",
    "                if len(q) <= 2:\n",
    "                    continue\n",
    "                \n",
    "                prompt_sample = Augment.get_prompt(self.mode, retriever, q + \"?\", prev_q=prev_q).prompt\n",
    "                output = self.generate_text(prompt_sample)\n",
    "                seperate_output.append(output)\n",
    "                prev_q = q\n",
    "\n",
    "                answer = \" \".join(seperate_output)\n",
    "                responses.append(answer)\n",
    "\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool('false')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from ../model_checkpoints/gemma/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =   500.25 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 617\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "generator = Generator.from_config('../config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'cpu-gemini',\n",
       " 'model': <llama_cpp.llama.Llama at 0x73fa3c946a10>,\n",
       " 'tokenizer': None}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = generator.get_generative_model()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = generator.get_tokenizer()\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = generator.get_text_generation_pipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?',\n",
       " '도배지에 녹은 자국이 발생하는 주된 원인과 그 해결 방법은 무엇인가요?',\n",
       " '큐블럭의 단점을 알려주세요. 또한, 압출법 단열판을 사용하는 것의 장점은 무엇인가요?',\n",
       " '철골구조를 사용하는 고층 건물에서, 단열 효과를 높이기 위한 시공 방법은 무엇이 있을까요?',\n",
       " '도배지의 완전한 건조를 위해 몇 주 동안 기다려야 하나요?',\n",
       " '철근철골콘크리트 구조가 적용된 건물의 장단점은 무엇인가요?',\n",
       " '어떤 상황에 개별 공간이 더 적합한지, 어떤 상황에 오픈 플랜 공간이 더 적합한지 알려주세요. 그리고 합지벽지의 어떤 단점이 있나요?',\n",
       " '도배지 들뜸 현상이 발생하는 가장 일반적인 원인은 무엇인가요?',\n",
       " '도배지에 얼룩이 생기는 다양한 원인들에 대해서 자세히 알려주세요.',\n",
       " '고층 건물을 건축할 때 철골구조가 주로 선택되는 이유는 무엇인가요?',\n",
       " '유성페인트의 환경 오염에 대한 예시를 알려주세요.',\n",
       " '훼손과 오염은 어떻게 다른가요? 또한, 부실 시공으로 인해 타공하자가 발생할 가능성이 있나요?',\n",
       " '철골콘크리트 구조에서 사용되는 철근의 재료적 특징은 무엇인가요?\"',\n",
       " '방염벽지가 실크벽지와 합지벽지와 다른 점은 무엇인가요?',\n",
       " '도배지에 곰팡이가 생겼을 때 높은 습도가 원인인 경우, 습기 관리는 어떻게 해야 할까요?\"',\n",
       " '롱브릭타일은 어떤 종류의 타일인가요? 그리고 페인트 상도재의 역할과 사용 방법에 대해 알려주세요.',\n",
       " '준불연재료는 무엇인가요? 그리고 유성페인트를 사용하는 것에 대한 부작용이 있을까요?',\n",
       " '어떤 환경 요인이 몰딩 수정을 유발할 수 있는가요? 그리고 반점이 생긴지 1년 이내인 하자에 대해 어떤 보수작업을 해야 하나요?',\n",
       " '벽에 뚫린 구멍이 이상하다고 하셨는데, 타공 불량이 발생하는 원인과 그 해결 방법에 대해 자세히 설명해주실 수 있을까요?\"',\n",
       " '다이닝 룸을 더 고급스럽게 꾸밀려면 어떻게 해야 하나요와 아레카 야자를 키우면 어떤 이점이 있는지의 관계는 무엇인가요?',\n",
       " '습도가 높아서 석고수정이 발생하나요? 또한, 피스 하자가 무엇인지 자세히 설명해 주실 수 있나요?',\n",
       " '공동주택의 실내 공기질을 측정할 때, 어떤 층에서 샘플을 채취하는 것이 가장 적절한가요?\"',\n",
       " '타일 바닥에서 파손된 타일을 교체하는 과정에 어떤 단계가 포함되나요? 또한, 겨울에 도배를 할 때 특별히 신경써야 할 사항이 있을까요?',\n",
       " '유성페인트를 사용하는 것에 대한 부작용이 있을까요? 또한, 페인트가 남으면 어디에 보관하는 게 좋을까요?',\n",
       " '가장 비싼 바닥재는 어떤 종류인가요? 또한, 유광 자기질 타일은 얼마나 오랫동안 사용할 수 있을까요?',\n",
       " '풍수지리를 활용하여 집을 꾸밀 때 어떤 주의사항을 고려해야 할까요? 그리고 스탠드조명을 선택할 때 주의할 점이 있을까요?',\n",
       " '반점이 1년 이상 생긴 하자를 보수하는 방법은 무엇인가요? 그리고 불량 도배지를 사용할 경우 도배지가 얼마나 오랫동안 버틸까요?',\n",
       " '평지붕의 단점은 무엇인가요? 그리고 실크벽지의 교체 주기는 얼마나인가요?',\n",
       " '경량철골구조는 어떤 건물에 사용되는 건축 구조물인가요? 그리고 철골구조의 장점은 무엇인가요?',\n",
       " '프리케스트 콘크리트 구조에 대해 자세히 설명해 주실 수 있나요? 그리고 조적식 구조란 무엇인가요?',\n",
       " '반려동물을 위한 가구로 낮은 높이의 가구와 패브릭 소재의 가구가 선택되는 이유는 무엇인가요?',\n",
       " '몰딩 수정을 예방하기 위해 건물 내부에서 어떤 종류의 환경 관리가 필요한가요?',\n",
       " 'KMEW 세라믹 지붕재의 단점에 대해 알려주세요. 또한, 세라믹 타일을 사용할 때 고려해야 할 단점은 무엇인가요?',\n",
       " '줄퍼티 마감은 무엇인가요? 또한, 액체방수공사는 무엇을 하는 것인가요?',\n",
       " '페인트 하도재 없이 페인트를 바로 칠할 경우 어떤 문제가 발생할 수 있나요?',\n",
       " '바닥재가 남으면 어떻게 처리하는 게 좋을까요? 그리고 장판이 남을 때 어떻게 처리해야 하나요?',\n",
       " '도배지에 생긴 반점을 없애기 위해 가장 효과적인 방법은 무엇인가요?',\n",
       " '새집증후군의 주요 원인은 무엇인가요?',\n",
       " '방청도료 도장 작업을 위해 필요한 단계는 무엇인가요? 또한, 콘크리트 벽에 구멍을 뚫는 방법에는 어떤 도구나 기술을 사용해야 하나요?',\n",
       " '어떤 종류의 실내 식물을 선택해야 식물을 효과적으로 가꾸는 데 도움이 될까요? 그리고 인테리어에 가장 많이 사용되는 도배재료는 무엇인가요?',\n",
       " '원목마루와 롱브릭타일에 대해 설명해주세요. 각각의 단점과 특징은 무엇인가요?',\n",
       " '침실을 더 아늑하게 꾸밀 수 있는 방법에는 어떤 것이 있을까요와 아이가 있는 집을 꾸밀 때 안전을 위해 고려해야 할 요소는 무엇인가요의 관계는 무엇인가요?',\n",
       " '인테리어에서 컬러매치를 위한 효과적인 팁이 있을까요? 그리고 복도나 협소한 공간을 확장시키기 위해 가장 효과적인 방법이 무엇일까요?',\n",
       " '그라스울 보온판의 주요 장점 중 하나인 흡음 성능은 어떻게 발휘될까요?\"',\n",
       " '미네랄울 보온판은 왜 고속 회전원심공법으로 제조되는 건가요?\"',\n",
       " '도배 후 필름 시공은 어떤 경우에 추천하시나요? 또한, 낡은 목재 가구의 흠집을 숨기는 방법을 알려주세요.',\n",
       " '입구나 복도의 표면에 사용하기 적합한 페인트 종류는 무엇이며, 이 공간을 환영스럽게 꾸미는 데 있어 어떤 인테리어 요소가 중요한가요?',\n",
       " '도배지가 먼지나 연기로 인해 얼마나 빨리 오염될 수 있나요? 그리고 습도가 높을 때 곰팡이가 어떻게 발생하는지 자세히 알고 싶습니다.',\n",
       " '방청페인트를 시공하는 방법에는 어떤 단계가 포함되나요? 또한, 배관공사 시 통기구를 설치해야 하는 이유가 무엇인가요?',\n",
       " '유성발수제를 사용하는 것의 실제 효과는 무엇인가요? 또한, 규산질계 침투성 도포 방수공사는 어떤 방식으로 이루어지나요?',\n",
       " '높은 습도로 인해 몰딩수정이 발생하는 경우가 있을까요? 또한, 내부와 외부 온도의 큰 차이로 인해 곰팡이 발생이 빨라지나요?',\n",
       " '인테리어 소품을 선택할 때 어떤 요소에 주의해야 할까요와 주방을 활기차게 꾸미기 위해 어떤 요소를 추가할 수 있을까요의 관계는 무엇인가요?',\n",
       " '초배지만 남은 벽에 페인트를 칠하면 어떤 문제가 발생하나요? 또한, 속건형 유성 발수제의 사용 목적과 효과에 대해 알려주세요.',\n",
       " '벽지에 반점이 생겼을 때, 왜 1년 이내인 경우에만 벽지 아세톤 용제 함침 방법을 사용하고 개선 벽지로 재시공해야 하나요?',\n",
       " '석구조란 무엇인가요? 그리고 기둥-보 구조 방식은 무엇을 의미하나요?',\n",
       " '원목마루의 어떤 단점이 있는지 알려주세요. 그리고 도배지가 남으면 어떻게 처리해야 하나요?',\n",
       " '마감재의 하자를 판단하는 데 어떤 방법을 사용해야 할까요? 그리고 새집증후군을 예방하는 데 가장 효과적인 방법은 무엇인가요?',\n",
       " '강마루 바닥재의 장점은 무엇인가요?',\n",
       " '새집증후군을 예방하기 위해 창문을 열어 환기하는 이유는 무엇인가요?',\n",
       " '도배풀을 제거하는 데 어떤 도구가 가장 효과적인가요? 또한, 옥상 방수용 탄성 에멀전 페인트를 사용하는 장점은 무엇인가요?',\n",
       " '통나무구조 방식의 건물에서 침하 현상을 최소화하기 위해 어떤 디테일을 고려해야 하나요?',\n",
       " 'MSDS(Material Safety Data Sheet)가 필요한 이유는 무엇인가요?',\n",
       " '인테리어 디자인에서 에나멜 계열 페인트를 사용하여 공간의 색상을 선택할 때 고려해야 할 요소들은 무엇인가요?',\n",
       " '라돈을 측정하는 데 가장 적합한 지점은 어디인가요? 그리고 MSDS(Material Safety Data Sheet)는 무엇을 포함하고 있나요?',\n",
       " '새집증후군을 해결하기 위한 방법에는 어떤 것들이 있나요? 그리고 소화기 종류에는 어떤 것들이 있는지 알려주세요.',\n",
       " '질석벽지가 아트월이나 현관 입구에서 많이 사용되는 이유는 무엇인가요?',\n",
       " '석고보드를 이동하면 도배지 꼬임이 발생할 가능성이 있나요? 또한, 건조시간이 충분하지 않으면 도배지가 꼬일 수 있는 이유가 무엇인가요?',\n",
       " '폴리에스테르 흡음 단열재의 장점 중 하나인 \"배수기능\"은 어떤 종류의 공간에 사용하기에 적합한가요?',\n",
       " '도배 후 제조사 권장 건조시간을 지키지 않으면 어떤 문제가 발생할 수 있나요?',\n",
       " '내단열 방식에서 발생하는 열교 문제와 곰팡이 발생의 원인은 무엇인가요?',\n",
       " '도배지가 찢어진 경우 터짐 하자가 발생하는 원인과 그에 따른 책임소재는 무엇인가요? 그리고 이를 해결하기 위해 어떤 방법을 사용할 수 있나요?',\n",
       " '건물의 면진장치는 지진 발생 시 어떻게 작동하나요?',\n",
       " '페인트 상도재의 역할과 사용 방법에 대해 알려주세요. 또한, 강화마루의 장점은 무엇입니까?',\n",
       " '경질우레탄폼 보온판은 왜 경제적인 선택인가요?',\n",
       " '상도작업은 어떻게 이뤄지나요? 또한, 공간을 넓게 보이도록 인테리어를 꾸미는 방법은 뭐가 있을까요?',\n",
       " '도배지가 울음 현상을 보일 때 그 원인과 대처 방법은 무엇인가요?',\n",
       " '사무실 실내공기 측정을 위한 적절한 위치는 어디일까요? 그리고 아파트 도배 평수를 어떻게 계산해야 하나요?',\n",
       " '도배지에 고습도로 인해 생기는 얼룩을 제거하는 방법이 있을까요? 또한, 구조적 결함 때문에 석고수정이 발생할 가능성이 있는가요?',\n",
       " '대리석 타일이 난방 절감에 어떤 역할을 하는 건가요?',\n",
       " '외단열 시공 시 외부 환경 조건이 중요한 이유는 무엇인가요?',\n",
       " '스탠드 조명을 선택할 때 어떤 조명이 적합한가요?',\n",
       " '실크벽지의 얼룩을 지워내는 가장 효과적인 방법이 있을까요? 그리고 부적절한 설치로 인해 제품의 품질에 영향을 미칠 수 있을까요?',\n",
       " '차음재의 차음 성능은 어떻게 평가되나요?',\n",
       " '공동주택의 실내 공기질을 측정하기 위한 적절한 지점은 어디일까요? 그리고 마감재의 하자를 판단하는 데 어떤 방법을 사용해야 할까요?',\n",
       " '실크벽지의 얼룩을 제거하기 위한 다양한 방법 중에서 특히 효과적인 방법은 무엇인가요?',\n",
       " '페인트 처리 시 페인트의 양에 따라 어떤 절차를 따라야 하나요?',\n",
       " '건설 산업은 어떤 종류의 작업을 포함하는지, 그리고 토목이 무엇인지 설명해줘.',\n",
       " '제진구조는 건물 구조 중에서 어떤 역할을 하는 건가요? 그리고 중목구조 방식에 대해 좀 더 자세히 설명해 주실 수 있을까요?',\n",
       " '작은 공간의 홈오피스에 원목사이딩을 사용할 때 고려해야 할 단점과 이를 극복하기 위한 인테리어 팁은 무엇인가요?',\n",
       " '알루미늄징크의 단점에 대해서 자세히 알려주세요 또한, 아이소핑크의 장점은 무엇인가요?',\n",
       " '겨울철 도배 작업에서 실내 온도를 일정하게 유지해야 하는 이유는 무엇인가요?',\n",
       " '철골구조의 화재 안전 보완을 위한 다양한 방법에는 무엇이 있을까요? 그리고 벽돌구조란 무엇인가요?',\n",
       " '석구조란 무엇인가요? 그리고 기둥-보 구조 방식은 무엇을 의미하나요?',\n",
       " '포세린 타일을 사용하는 것에 대한 단점이 무엇인가요? 그리고 셀룰로오스의 단점에 대해 설명해주세요.',\n",
       " '부드러운 욕실 인테리어를 위해 사용할 수 있는 소재나 색상은 어떤 것들이 있을까요? 그리고 반려동물을 위한 바닥재에는 어떤 종류가 있는지 알려주세요.',\n",
       " '팬던트 라이트는 무엇이며, 어떤 용도로 사용되나요? 그리고 인테리어에서 바닥재를 선택할 때 고려해야 할 중요한 요소는 무엇인가요?',\n",
       " '속건형 유성 발수제가 건물의 수명을 연장시키는 원리는 무엇인가요?',\n",
       " '주방에서 조리할 때 어떤 종류의 조명이 가장 적합한가요?',\n",
       " '인테리어에서 생기 넘치는 식물과 아트워크를 부착할 때 밀풀을 사용하는 것이 좋은가요, 그리고 밀풀 사용 시 주의할 점은 무엇인가요?',\n",
       " '철근콘크리트 구조에 대해 좀 더 자세히 알려주세요. 그리고 통나무구조 방식은 어떤 건물에 사용되는 건축 구조 방식인가요?',\n",
       " '공명형 흡음재가 특정 주파수에 한정적으로 적용되는 이유는 무엇인가요?\"',\n",
       " '결로의 발생 원인은 무엇이고, 이를 방지하기 위해 어떤 시공방법을 사용해야 하나요?',\n",
       " '베란다를 활용하여 야외 라운지를 만들기 위해 필요한 장비나 소품은 어떤 것이 있을까요와 거실에 현대적인 느낌을 주기 위한 팁은 무엇인가요의 관계는 무엇인가요?',\n",
       " '페인트 작업 시 유해물질을 최소화하기 위해 어떤 점을 유의해야 하나요?\"',\n",
       " '흡음재 중 판 진동형의 작동 원리는 무엇인가요?\"',\n",
       " '점토벽돌을 사용하는 것의 장점은 무엇인가요? 또한, 도료와 벽지 중에서 어떤 것을 선택하는 것이 더 나은 선택일까요?',\n",
       " '강화마루는 어떤 재료로 제작되나요? 또한, 징크판넬의 단점에는 어떤 것들이 있을까요?',\n",
       " '벽에 타공하자가 발생했을 때, 이로 인해 벽장 부위에 결로가 생길 수 있는지, 그리고 이를 방지하기 위한 방법은 무엇인가요?',\n",
       " '물 누수로 인한 곰팡이와 곰팡이 냄새를 제거하는 데 사용할 수 있는 가정용 청소용품이 있을까요? 그리고 펫테리어가 무엇인지 자세히 알려주세요.',\n",
       " '도배지를 너무 작게 자르면 어떤 문제가 발생할 수 있을까요? 그리고 높은 습도로 인해 도배지 패턴이 이어지지 않을 수 있는 이유가 무엇일까요?',\n",
       " '알러지가 있는 사람들이 매끄럽고 평평한 벽지를 선택하는 이유는 무엇인가요?',\n",
       " '아파트 도배 평수를 계산하는 방법과 소화기 설치 시 주의해야 할 사항에 대해 설명해줄 수 있나요?',\n",
       " '마감재의 하자를 판단하는 데 어떤 방법을 사용해야 할까요? 그리고 라돈을 측정하는 데 가장 적합한 지점은 어디인가요?',\n",
       " '이산화탄소 소화기에 대해 좀 더 자세히 알 수 있을까요? 그리고 분말 소화기는 어떤 용도로 사용되는 건가요?',\n",
       " '면진장치는 어떤 용도로 사용되는 건가요? 그리고 면진구조는 어떤 건물 구조나 시스템을 의미하나요?',\n",
       " '외단열재가 실내 온도 유지에 어떻게 도움을 주며, 이를 시공할 때 고려해야 할 주요 장점들은 무엇인가요?',\n",
       " '외단열과 내단열의 차이점은 무엇이며, 각각의 시공방법이 어떻게 단열 효과에 영향을 미치나요?',\n",
       " '바닥재의 종류 중에서 가장 인기 있는 것은 무엇이며, 가장 비싼 바닥재는 무엇인가요?',\n",
       " '포세린 타일의 장점과 단점은 무엇인가요?',\n",
       " '중목구조 방식에 대해 좀 더 자세히 설명해 주실 수 있을까요? 그리고 철근콘크리트 구조에 대해 좀 더 자세히 알려주세요.',\n",
       " '도배지는 얼마나의 양이 필요한가요? 또한, 벽에 구멍을 막는 가장 효과적인 방법은 무엇인가요?',\n",
       " '경질우레탄폼 보온판을 사용한 외단열 시공은 어떤 장점을 가지며, 이를 통해 어떻게 결로와 곰팡이 발생을 줄일 수 있나요?',\n",
       " '질석벽지가 아트월이나 현관 입구에 많이 사용되는 이유는 무엇인가요?',\n",
       " '시트 방수공사를 통해 건물의 에너지 절감 효과를 어떻게 얻을 수 있나요?\"',\n",
       " '내진설계에서 안정성을 높이기 위한 순서는 무엇인가요? 그리고 내진구조란 무엇인가요?',\n",
       " '분말 소화기를 사용할 때 주의해야 할 사항은 무엇인가요? 그리고 아파트 도배 평수를 어떻게 계산해야 하나요?',\n",
       " '압출법 보온판의 가장 큰 장점은 무엇인가요?\"',\n",
       " '평지붕의 누수 문제를 방지하기 위해 수성 벽체용 탄성 방수 도료를 사용하는 것이 어떤 장점이 있나요?',\n",
       " '석고수정이 발생하는 가장 큰 원인은 무엇인가요? 그리고 이를 해결하는 방법에 대해 알려주세요.\"',\n",
       " '카페트의 기대 수명은 얼마나 될까요? 그리고 오리지널징크의 장점에는 무엇이 있나요?']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a= '/home/bibekali/dacon-hansol-deco-challenge/hansolrag/data/test.csv'\n",
    "test=pd.read_csv(a)\n",
    "list(test.질문)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   51384.35 ms\n",
      "llama_print_timings:      sample time =      65.03 ms /    40 runs   (    1.63 ms per token,   615.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   22334.64 ms /    40 runs   (  558.37 ms per token,     1.79 tokens per second)\n",
      "llama_print_timings:       total time =   23410.13 ms /    41 tokens\n",
      "1it [00:44, 44.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list = [\"면진장치가 뭐야?\"]\n",
    "generator.get_output(retriever, query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = \"\"\"\n",
    "벽지에 대한 고객 문의에 정확하고 유용한 답변을 작성한다. <질문>의 의도를 파악하여 정확하게 <보고서>만을 기반으로 답변하세요.\n",
    "<보고서>\n",
    "TRAIN_000, 면진장치가 뭐야?, 면진장치에 사용되는 주요 기술은 무엇인가요?, 건축구조, 면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다., 면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다., 면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다., 면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다., 면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.\n",
    "</보고서>\n",
    "지침사항을 반드시 지키고, <보고서>를 기반으로 <질문>에 답변하세요.\n",
    "<질문>\n",
    "면진장치가 뭐야?\n",
    "</질문>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansol-deco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
